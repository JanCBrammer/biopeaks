import asyncio
from io import StringIO
from timeit import default_timer as timer
import aiohttp
import numpy as np
from wfdb.processing import compare_annotations


class BenchmarkDetectorGUDB:
    """Evaluate an ECG R-peak detector on datasets from the GUDB database.
    """

    def __init__(self, detector, tolerance, sfreq=250, n_runs=100):
        """
        Parameters
        ----------
        detector : function
            A function that takes a 1d array containing a physiological record
            as first positional argument and an integer sampling rate as second
            positional argument. Must return a 1d array containing the detected
            extrema.
        tolerance : int
            Maximum difference in samples that is permitted between the manual
            annotation and the annotation generated by the detector.
        sfreq : int, optional
            The sampling rate of the GUDB records in Hertz, by default 250.
        n_runs : int, optional
            The number of runs used for obtaining the average run time of the
            detector, by default 100.
        """
        self.detector = detector
        self.tolerance = tolerance
        self.sfreq = sfreq
        self.n_runs = n_runs
        self.session = None
        self.queue = None


    async def score_record(self, record, annotation):
        """Obtain detector performance for an annotated record.

        Parameters
        ----------
        record : 1d array
            The raw physiological record.
        annotation : 1d array
            The manual extrema annotations.

        Returns
        -------
        precision : float
            The detectors precision on the record given the tolerance.
        sensitivity : float
            The detectors sensitivity on the record given the tolerance.

        """
        detector_annotation = self.detector(record, self.sfreq)

        comparitor = compare_annotations(detector_annotation, annotation,
                                         self.tolerance)
        tp = comparitor.tp
        fp = comparitor.fp
        fn = comparitor.fn
        sensitivity = tp / (tp + fn)
        precision = tp / (tp + fp)

        return precision, sensitivity


    async def time_record(self, record):
        """Obtain the average run time of a detector on a record over N runs.

        Parameters
        ----------
        record : 1d array
            The raw physiological record.

        Returns
        -------
        avg_time : int
            The run time of the detector on the record averaged over n_runs. In
            milliseconds.

        """
        start = timer()

        for _ in range(self.n_runs):
            self.detector(record, self.sfreq)

        end = timer()
        avg_time = (end - start) / self.n_runs * 1000

        return avg_time


    async def fetch_record(self, url):
        """Get a record from the GUDB server.

        Fetch the raw physiological data and the corresponding annotation,
        format them, and put them on a queue for further processing.

        Parameters
        ----------
        url : str
            An experiment directory of the GUDB. The URL must end with the
            experiment ID. E.g., "URL/maths". The experiment ID can be one of
            {"maths", "hand_bike", "jogging", "walking", "sitting"}.
        """
        print(f"fetching {url}")
        async with self.session.get(url + "/ECG.tsv") as response:
            if response.status == 200:
                physio = await response.text()
                physio = np.loadtxt(StringIO(physio))
                physio = np.ravel(physio[:, 1])    # select the first lead
            else:
                physio = None
                print(f"Couldn't find physio file at {url}")
        async with self.session.get(url + "/annotation_cables.tsv") as response:
            if response.status == 200:
                annotation = await response.text()
                annotation = np.loadtxt(StringIO(annotation))
                annotation = np.ravel(annotation)
            else:
                annotation = None
                print(f"Couldn't find annotation file at {url}")
        await self.queue.put((physio, annotation, url))


    async def benchmark_record(self, n_records):
        """Evaluate the performance of the detector on a single record.

        Parameters
        ----------
        n_records : int
            Overall number of records to be evaluated. Necessary to know when
            to stop waiting for incoming records.
        """
        n = 0
        while n < n_records:
            # Wait for a record to be added to the queue.
            physio, annotation, url = await self.queue.get()

            skip_record = physio is None
            skip_record = annotation is None

            if skip_record:
                print(f"Skipping benchmarking of {url}.")
                n += 1
                continue

            # Process the record.
            print(f"processing {url}")
            precision, sensitivity = await self.score_record(physio, annotation)
            avg_time = await self.time_record(physio)
            print(precision, sensitivity, avg_time)

            n += 1


    async def _benchmark_records(self, experiment_urls):
        """Evaluate the performance of the detector on a set of records.

        Parameters
        ----------
        experiment_urls : list
            List of strings containing the URLs to GUDB records. The URL must
            end with the experiment ID. E.g., "URL/maths". The experiment ID can
            be one of {"maths", "hand_bike", "jogging", "walking", "sitting"}.
        """
        self.session = aiohttp.ClientSession()
        self.queue = asyncio.Queue()
        fetch_coro = [self.fetch_record(url) for url in experiment_urls]
        benchmark_coro = self.benchmark_record(len(experiment_urls))

        await asyncio.gather(*fetch_coro, benchmark_coro)
        await self.session.close()


    def benchmark_records(self, experiment_urls):
        """Wrapper starting the event loop."""
        asyncio.run(self._benchmark_records(experiment_urls))
